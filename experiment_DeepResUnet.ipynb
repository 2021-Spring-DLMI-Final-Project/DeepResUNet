{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "experiment-DeepResUnet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "MQLhkrxnr8tz"
      },
      "source": [
        "# Experiment Deep ResUNet\n",
        "#### Setup:\n",
        "**Dataset**: 3 different types of brain tumors <br>\n",
        "**Loss function**: Dice loss <br>\n",
        "**Optimizer**: Adam <br>\n",
        "**Model**: Deep ResUNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48n9XBPLMaSC"
      },
      "source": [
        "### Check if you are using GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2bmg9GC-HFa",
        "outputId": "6ad0e63c-7b60-48b3-a128-6b9c307d6322"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed May 19 15:59:21 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P0    30W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwh03HItMg7c"
      },
      "source": [
        "### Mount Directory to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQxTY6lsMYTv",
        "outputId": "9f6718ba-c63f-4123-b65e-fb7bb9e643d7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import os\n",
        "\n",
        "# your workspace in your drive\n",
        "workspace = 'DLMI_Final'\n",
        "\n",
        "\n",
        "try:\n",
        "  os.chdir(os.path.join('/content/gdrive/My Drive/', workspace))\n",
        "except:\n",
        "  os.mkdir(os.path.join('/content/gdrive/My Drive/', workspace))\n",
        "  os.chdir(os.path.join('/content/gdrive/My Drive/', workspace))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfDdHVBEOztL"
      },
      "source": [
        "### Fix Random Seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZeZUAl5O54K"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "\n",
        "def same_seeds(seed):\n",
        "    # Python built-in random module\n",
        "    random.seed(seed)\n",
        "    # Numpy\n",
        "    np.random.seed(seed)\n",
        "    # Torch\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "same_seeds(36963)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXD4mGh3Fkx7"
      },
      "source": [
        "### Import Basic Package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "oxW-U0SWr8t5"
      },
      "source": [
        "# Imports and training parameters\n",
        "import torch\n",
        "from torchvision.transforms import transforms\n",
        "#from src.datasets import BrainTumorDatasetMask\n",
        "#from src.loss import  dice_loss\n",
        "#from src.metrics import dice_coeff\n",
        "#from src.models import  DeepResUNet\n",
        "#from src.trainers import Trainer\n",
        "torch.manual_seed(0)\n",
        "num_epochs = 100\n",
        "batch_size = 2\n",
        "learning_rate = 0.001\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvQAY1WVFPK8"
      },
      "source": [
        "### Implement Brain Tumor Dataset (with Mask)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRmEX-35EFx0",
        "outputId": "3b9f596f-d6c2-43a0-c19a-e2ac48eff3bf"
      },
      "source": [
        "!pip install hdf5storage"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hdf5storage\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/fe/3eccf8e076e5458314205f315eb0702cfe4fcb533944726a4910edcec2c6/hdf5storage-0.1.18-py2.py3-none-any.whl (53kB)\n",
            "\r\u001b[K     |██████▏                         | 10kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 20kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 30kB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 40kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 51kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py>=2.1; python_version >= \"3.3\" in /usr/local/lib/python3.7/dist-packages (from hdf5storage) (2.10.0)\n",
            "Requirement already satisfied: numpy; python_version >= \"3.4\" in /usr/local/lib/python3.7/dist-packages (from hdf5storage) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py>=2.1; python_version >= \"3.3\"->hdf5storage) (1.15.0)\n",
            "Installing collected packages: hdf5storage\n",
            "Successfully installed hdf5storage-0.1.18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0SCMPgsEKAL"
      },
      "source": [
        "import urllib.request\n",
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "import hdf5storage\n",
        "\n",
        "class DownloadProgressBar(tqdm):\n",
        "    def update_to(self, b=1, bsize=1, tsize=None):\n",
        "        if tsize is not None:\n",
        "            self.total = tsize\n",
        "        self.update(b * bsize - self.n)\n",
        "\n",
        "def download_url(url, target_folder, filename):\n",
        "    # check if data exists\n",
        "    print(\"Check if data exists on disk\")\n",
        "    if not os.path.isdir(target_folder):\n",
        "      print(\"Creating target folder\")\n",
        "      os.mkdir(target_folder)\n",
        "    files = os.listdir(target_folder)\n",
        "    if not files:\n",
        "        print(\"Cannot find files on disk\")\n",
        "        print(\"Downloading files\")\n",
        "        with DownloadProgressBar(unit='B', unit_scale=True,\n",
        "                                 miniters=1, desc=url.split('/')[-1]) as t:\n",
        "            urllib.request.urlretrieve(url, filename=target_folder + filename, reporthook=t.update_to)\n",
        "    print(\"Download completed!\")\n",
        "\n",
        "def unzip_all_files(target_folder):\n",
        "    print(\"Unzip files\")\n",
        "    items = os.listdir(target_folder)\n",
        "    while(any(item.endswith('.zip') for item in items)):\n",
        "        for item in filter(lambda item: item.endswith('.zip'), items):\n",
        "            with zipfile.ZipFile(target_folder + item, \"r\") as zip_ref:\n",
        "                zip_ref.extractall(target_folder)\n",
        "        for item in items:\n",
        "            if item.endswith(\".zip\"):\n",
        "                os.remove(target_folder + item)\n",
        "        items = os.listdir(target_folder)\n",
        "    print(\"Unzip completed!\")\n",
        "\n",
        "def convert_landmark_to_bounding_box(landmark):\n",
        "    x_min = x_max = y_min = y_max = None\n",
        "    for x, y in landmark:\n",
        "        if x_min is None:\n",
        "            x_min = x_max = x\n",
        "            y_min = y_max = y\n",
        "        else:\n",
        "            x_min, x_max = min(x, x_min), max(x, x_max)\n",
        "            y_min, y_max = min(y, y_min), max(y, y_max)\n",
        "    return [int(x_min), int(x_max), int(y_min), int(y_max)]\n",
        "\n",
        "def _arrange_brain_tumor_data(root):\n",
        "    # Remove and split files\n",
        "    items = [item for item in filter(lambda item: re.search(\"^[0-9]+\\.mat$\", item), os.listdir(root))]\n",
        "    try:\n",
        "        os.mkdir(root + 'meningioma/')\n",
        "    except:\n",
        "        print(\"Meningioma directory already exists\")\n",
        "    try:\n",
        "        os.mkdir(root + 'glioma/')\n",
        "    except:\n",
        "      print(\"Glioma directory already exists\")\n",
        "    try:\n",
        "        os.mkdir(root + 'pituitary/')\n",
        "    except:\n",
        "        print(\"Pituitary directory already exists\")\n",
        "\n",
        "    for item in items:\n",
        "        sample = hdf5storage.loadmat(root + item)['cjdata'][0]\n",
        "        if sample[2].shape[0] == 512:\n",
        "            if sample[0] == 1:\n",
        "                os.rename(root + item, root + 'meningioma/' + item)\n",
        "            if sample[0] == 2:\n",
        "                os.rename(root + item, root + 'glioma/' + item)\n",
        "            if sample[0] == 3:\n",
        "                os.rename(root + item, root + 'pituitary/' + item)\n",
        "        else:\n",
        "            os.remove(root + item)\n",
        "\n",
        "def get_data_if_needed(data_path='./data/', url=\"https://ndownloader.figshare.com/articles/1512427/versions/5\"):\n",
        "    if os.path.isdir(data_path):\n",
        "        print(\"Data directory already exists. \",\n",
        "              \"if from some reason the data directory structure is wrong please remove the data dir and rerun this script\")\n",
        "        return\n",
        "    filename = \"all_data.zip\"\n",
        "    download_url(url, data_path, filename)\n",
        "    unzip_all_files(data_path)\n",
        "    _arrange_brain_tumor_data(data_path)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fuf0wH88uRQJ"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "#from src.utils.data import get_data_if_needed, convert_landmark_to_bounding_box\n",
        "from torchvision.transforms import ToPILImage\n",
        "import os\n",
        "import math\n",
        "import hdf5storage\n",
        "from enum import Enum\n",
        "\n",
        "\n",
        "class ClassesLabels(Enum):\n",
        "    Meningioma = 1\n",
        "    Glioma = 2\n",
        "    Pituitary = 3\n",
        "\n",
        "    def __len__(self):\n",
        "        return 3\n",
        "\n",
        "\n",
        "class BrainTumorDataset(Dataset):\n",
        "    \"\"\"\n",
        "        This brain tumor dataset containing 3064 T1-weighted contrast-inhanced images\n",
        "        from 233 patients with three kinds of brain tumor: meningioma (708 slices),\n",
        "        glioma (1426 slices), and pituitary tumor (930 slices). Due to the file size\n",
        "        limit of repository, we split the whole dataset into 4 subsets, and achive\n",
        "        them in 4 .zip files with each .zip file containing 766 slices.The 5-fold\n",
        "        cross-validation indices are also provided.\n",
        "\n",
        "        -----\n",
        "            This data is organized in matlab data format (.mat file). Each file stores a struct\n",
        "            containing the following fields for an image:\n",
        "\n",
        "            label: 1 for meningioma, 2 for glioma, 3 for pituitary tumor\n",
        "            PID: patient ID\n",
        "            image: image data\n",
        "            tumorBorder: a vector storing the coordinates of discrete points on tumor border.\n",
        "                    For example, [x1, y1, x2, y2,...] in which x1, y1 are planar coordinates on tumor border.\n",
        "                    It was generated by manually delineating the tumor border. So we can use it to generate\n",
        "                    binary image of tumor mask.\n",
        "            tumorMask: a binary image with 1s indicating tumor region\n",
        "\n",
        "        -----\n",
        "        taken from https://figshare.com/articles/brain_tumor_dataset/1512427 all right reserved to\n",
        "            Jun Cheng\n",
        "            School of Biomedical Engineering\n",
        "            Southern Medical University, Guangzhou, China\n",
        "            Email: chengjun583@qq.com\n",
        "        -----\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root, train=True, download=True,\n",
        "                                                  classes=(ClassesLabels.Meningioma,\n",
        "                                                  ClassesLabels.Glioma,\n",
        "                                                  ClassesLabels.Pituitary)):\n",
        "        super().__init__()\n",
        "        test_fr = 0.15\n",
        "        if download:\n",
        "            get_data_if_needed(root)\n",
        "        self.root = root\n",
        "        # List all data files\n",
        "        items = []\n",
        "        if ClassesLabels.Meningioma in classes:\n",
        "            items += ['meningioma/' + item for item in os.listdir(root + 'meningioma/')]\n",
        "        if ClassesLabels.Glioma in classes:\n",
        "            items += ['glioma/' + item for item in os.listdir(root + 'glioma/')]\n",
        "        if ClassesLabels.Meningioma in classes:\n",
        "            items += ['pituitary/' + item for item in os.listdir(root + 'pituitary/')]\n",
        "\n",
        "        if train:\n",
        "            self.items = items[0:math.floor((1-test_fr) * len(items)) + 1]\n",
        "        else:\n",
        "            self.items = items[math.floor((1-test_fr) * len(items)) + 1:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get the data item\n",
        "            label: 1 for meningioma, 2 for glioma, 3 for pituitary tumor\n",
        "            PID: patient ID\n",
        "            image: image data\n",
        "            tumorBorder: a vector storing the coordinates of discrete points on tumor border.\n",
        "                    For example, [x1, y1, x2, y2,...] in which x1, y1 are planar coordinates on tumor border.\n",
        "                    It was generated by manually delineating the tumor border. So we can use it to generate\n",
        "                    binary image of tumor mask.\n",
        "            tumorMask: a binary image with 1s indicating tumor region\n",
        "            And convert it to more convenient python dict object\n",
        "        :param idx: index of item between 0 to len(self.item) - 1\n",
        "        :return: dict - {label: int, image: matrix, landmarks: array of tuple (x, y), mask: matrix, bounding_box 4 size array of (x, y)}\n",
        "        \"\"\"\n",
        "        if not (0 <= idx <  len(self.items)):\n",
        "            raise IndexError(\"Idx out of bound\")\n",
        "\n",
        "        data = hdf5storage.loadmat(self.root + self.items[idx])['cjdata'][0]\n",
        "        # transform the tumor border to array of (x, y) tuple\n",
        "        xy = data[3]\n",
        "        landmarks = []\n",
        "        for i in range(0, len(xy), 2):\n",
        "            x = xy[i][0]\n",
        "            y = xy[i + 1][0]\n",
        "            landmarks.append((x, y))\n",
        "        mask = data[4]\n",
        "        data[2].dtype = 'uint16'\n",
        "        image_with_metadata = {\n",
        "            \"label\": int(data[0][0]),\n",
        "            \"image\": ToPILImage()(data[2]),\n",
        "            \"landmarks\": landmarks,\n",
        "            \"mask\": mask,\n",
        "            \"bounding_box\": convert_landmark_to_bounding_box(landmarks)\n",
        "        }\n",
        "        return image_with_metadata\n",
        "\n",
        "\n",
        "class BrainTumorDatasetMask(BrainTumorDataset):\n",
        "    def __init__(self, root, train=True, transform=None, classes=(ClassesLabels.Meningioma,\n",
        "                                                  ClassesLabels.Glioma,\n",
        "                                                  ClassesLabels.Pituitary)):\n",
        "        super().__init__(root, train, classes=classes)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = super().__getitem__(idx)\n",
        "        sample = (item[\"image\"], item[\"mask\"])\n",
        "        return sample if self.transform is None else self.transform(*sample)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Noz4XbPFF2aJ"
      },
      "source": [
        "### Implement loss function of Dice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snWls9sQF0TZ"
      },
      "source": [
        "def dice_loss(pred, target, epsilon=1e-7, use_sigmoid=True):\n",
        "    pred = pred.contiguous()\n",
        "    if use_sigmoid:\n",
        "        pred = torch.sigmoid(pred)\n",
        "    target = target.contiguous()\n",
        "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
        "    loss = (1 - ((2. * intersection + epsilon) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + epsilon)))\n",
        "    return loss.mean()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sq13BZvgF885"
      },
      "source": [
        "def dice_coeff(pred, target, threshold=0.5, epsilon=1e-6, use_sigmoid = True):\n",
        "    # make sure the tensors are align in memory and convert to probabilities if needed\n",
        "    pred = pred.contiguous()\n",
        "    if use_sigmoid:\n",
        "        pred = torch.sigmoid(pred)\n",
        "    target = target.contiguous()\n",
        "\n",
        "    pred = (pred > threshold).float()\n",
        "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
        "    dice = (2. * intersection + epsilon) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + epsilon)\n",
        "    return dice.mean()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAMnkcRTHuwY"
      },
      "source": [
        "### Define DeepResUNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBCYRMlnH0Oo"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class PreActivateDoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(PreActivateDoubleConv, self).__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class PreActivateResBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(PreActivateResBlock, self).__init__()\n",
        "        self.ch_avg = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels))\n",
        "\n",
        "        self.double_conv = PreActivateDoubleConv(in_channels, out_channels)\n",
        "        self.down_sample = nn.MaxPool2d(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.ch_avg(x)\n",
        "        out = self.double_conv(x)\n",
        "        out = out + identity\n",
        "        return self.down_sample(out), out\n",
        "\n",
        "class PreActivateResUpBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(PreActivateResUpBlock, self).__init__()\n",
        "        self.ch_avg = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels))\n",
        "        self.up_sample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        self.ch_avg = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels))\n",
        "        self.double_conv = PreActivateDoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, down_input, skip_input):\n",
        "        x = self.up_sample(down_input)\n",
        "        x = torch.cat([x, skip_input], dim=1)\n",
        "        return self.double_conv(x) + self.ch_avg(x)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iIMNCRzGfxR"
      },
      "source": [
        "class DeepResUNet(nn.Module):\n",
        "    def __init__(self, out_classes=1):\n",
        "        super(DeepResUNet, self).__init__()\n",
        "\n",
        "        self.down_conv1 = PreActivateResBlock(1, 64)\n",
        "        self.down_conv2 = PreActivateResBlock(64, 128)\n",
        "        self.down_conv3 = PreActivateResBlock(128, 256)\n",
        "        self.down_conv4 = PreActivateResBlock(256, 512)\n",
        "\n",
        "        self.double_conv = PreActivateDoubleConv(512, 1024)\n",
        "\n",
        "        self.up_conv4 = PreActivateResUpBlock(512 + 1024, 512)\n",
        "        self.up_conv3 = PreActivateResUpBlock(256 + 512, 256)\n",
        "        self.up_conv2 = PreActivateResUpBlock(128 + 256, 128)\n",
        "        self.up_conv1 = PreActivateResUpBlock(128 + 64, 64)\n",
        "\n",
        "        self.conv_last = nn.Conv2d(64, out_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, skip1_out = self.down_conv1(x)\n",
        "        x, skip2_out = self.down_conv2(x)\n",
        "        x, skip3_out = self.down_conv3(x)\n",
        "        x, skip4_out = self.down_conv4(x)\n",
        "        x = self.double_conv(x)\n",
        "        x = self.up_conv4(x, skip4_out)\n",
        "        x = self.up_conv3(x, skip3_out)\n",
        "        x = self.up_conv2(x, skip2_out)\n",
        "        x = self.up_conv1(x, skip1_out)\n",
        "        x = self.conv_last(x)\n",
        "        return x"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07_bfunXJWtc"
      },
      "source": [
        "### Define Training Module(Trainer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TseZkr7gJIEI"
      },
      "source": [
        "import abc\n",
        "import os\n",
        "import sys\n",
        "import tqdm\n",
        "import torch\n",
        "import datetime\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import Callable, Any\n",
        "from typing import NamedTuple, List\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "def to_np(x):\n",
        "    return x.data.cpu().numpy()\n",
        "\n",
        "class BatchResult(NamedTuple):\n",
        "    \"\"\"\n",
        "    Represents the result of training for a single batch: the loss\n",
        "    and score of the batch.\n",
        "    \"\"\"\n",
        "    loss: float\n",
        "    score: float\n",
        "\n",
        "\n",
        "class EpochResult(NamedTuple):\n",
        "    \"\"\"\n",
        "    Represents the result of training for a single epoch: the loss per batch\n",
        "    and accuracy on the dataset (train or test).\n",
        "    \"\"\"\n",
        "    losses: List[float]\n",
        "    score: float\n",
        "\n",
        "\n",
        "class FitResult(NamedTuple):\n",
        "    \"\"\"\n",
        "    Represents the result of fitting a model for multiple epochs given a\n",
        "    training and test (or validation) set.\n",
        "    The losses are for each batch and the accuracies are per epoch.\n",
        "    \"\"\"\n",
        "    num_epochs: int\n",
        "    train_loss: List[float]\n",
        "    train_acc: List[float]\n",
        "    test_loss: List[float]\n",
        "    test_acc: List[float]\n",
        "    best_score: float\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"\n",
        "    A class abstracting the various tasks of training models.\n",
        "\n",
        "    Provides methods at multiple levels of granularity:\n",
        "    - Multiple epochs (fit)\n",
        "    - Single epoch (train_epoch/test_epoch)\n",
        "    - Single batch (train_batch/test_batch)\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 model,\n",
        "                 loss_fn,\n",
        "                 optimizer,\n",
        "                 objective_metric,\n",
        "                 device=\"cuda\",\n",
        "                 tensorboard_logger=None,\n",
        "                 tensorboard_log_images=True,\n",
        "                 experiment_prefix=None):\n",
        "        \"\"\"\n",
        "        Initialize the trainer.\n",
        "        :param model: Instance of the model to train.\n",
        "        :param loss_fn: The loss function to evaluate with.\n",
        "        :param optimizer: The optimizer to train with.\n",
        "        :param device: torch.device to run training on (CPU or GPU).\n",
        "        :param tensorboard_logger: tensordboard logger.\n",
        "        \"\"\"\n",
        "        self.tensorboard_logger = tensorboard_logger\n",
        "\n",
        "        if experiment_prefix is None:\n",
        "            now = datetime.datetime.now()\n",
        "            self.experiment_prefix = now.strftime(\"%Y-%m-%d\\%H:%M:%S\")\n",
        "        else:\n",
        "            self.experiment_prefix = experiment_prefix\n",
        "        self.tensorboard_log_images = tensorboard_log_images\n",
        "        self.model = model\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.objective_metric = objective_metric\n",
        "        self.device = device\n",
        "\n",
        "        if self.device:\n",
        "            model.to(self.device)\n",
        "\n",
        "    def fit(self, dl_train: DataLoader, dl_test: DataLoader,\n",
        "            num_epochs, checkpoints: str = None,\n",
        "            early_stopping: int = None,\n",
        "            print_every=1, **kw) -> FitResult:\n",
        "        \"\"\"\n",
        "        Trains the model for multiple epochs with a given training set,\n",
        "        and calculates validation loss over a given validation set.\n",
        "        :param dl_train: Dataloader for the training set.\n",
        "        :param dl_test: Dataloader for the test set.\n",
        "        :param num_epochs: Number of epochs to train for.\n",
        "        :param checkpoints: Whether to save model to file every time the\n",
        "            test set accuracy improves. Should be a string containing a\n",
        "            filename without extension.\n",
        "        :param early_stopping: Whether to stop training early if there is no\n",
        "            test loss improvement for this number of epochs.\n",
        "        :param print_every: Print progress every this number of epochs.\n",
        "        :return: A FitResult object containing train and test losses per epoch.\n",
        "        \"\"\"\n",
        "        try:\n",
        "          os.mkdir(checkpoints)\n",
        "        except:\n",
        "          print(\"checkpoints directory already exists\")\n",
        "\n",
        "        actual_num_epochs = 0\n",
        "        train_loss, train_acc, test_loss, test_acc = [], [], [], []\n",
        "\n",
        "        best_score = None\n",
        "        epochs_without_improvement = 0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            verbose = False  # pass this to train/test_epoch.\n",
        "            if epoch % print_every == 0 or epoch == num_epochs-1:\n",
        "                verbose = True\n",
        "            self._print(f'--- EPOCH {epoch+1}/{num_epochs} ---', verbose)\n",
        "\n",
        "            epoch_train_res = self.train_epoch(dl_train, verbose=verbose, **kw)\n",
        "            train_loss.extend([float(x.item()) for x in epoch_train_res.losses])\n",
        "            train_acc.append(float(epoch_train_res.score))\n",
        "\n",
        "            epoch_test_res = self.test_epoch(dl_test, verbose=verbose, **kw)\n",
        "            test_loss.extend([float(x.item()) for x in epoch_test_res.losses])\n",
        "            test_acc.append(float(epoch_test_res.score))\n",
        "\n",
        "            if best_score is None:\n",
        "                best_score = epoch_test_res.score\n",
        "            elif epoch_test_res.score > best_score:\n",
        "                best_score = epoch_test_res.score\n",
        "                if checkpoints is not None:\n",
        "                    torch.save(self.model, checkpoints)\n",
        "                    print(\"**** Checkpoint saved ****\")\n",
        "                epochs_without_improvement = 0\n",
        "            else:\n",
        "                if early_stopping is not None and epochs_without_improvement >= early_stopping:\n",
        "                    print(\"Early stopping after %s with out improvement\" % epochs_without_improvement)\n",
        "                    break\n",
        "                epochs_without_improvement += 1\n",
        "\n",
        "            # ========================\n",
        "\n",
        "        return FitResult(actual_num_epochs,\n",
        "                         train_loss, train_acc, test_loss, test_acc, best_score)\n",
        "\n",
        "    def train_epoch(self, dl_train: DataLoader, **kw) -> EpochResult:\n",
        "        \"\"\"\n",
        "        Train once over a training set (single epoch).\n",
        "        :param dl_train: DataLoader for the training set.\n",
        "        :param kw: Keyword args supported by _foreach_batch.\n",
        "        :return: An EpochResult for the epoch.\n",
        "        \"\"\"\n",
        "        self.model.train()  # set train mode\n",
        "        return self._foreach_batch(dl_train, self.train_batch, **kw)\n",
        "\n",
        "    def test_epoch(self, dl_test: DataLoader, **kw) -> EpochResult:\n",
        "        \"\"\"\n",
        "        Evaluate model once over a test set (single epoch).\n",
        "        :param dl_test: DataLoader for the test set.\n",
        "        :param kw: Keyword args supported by _foreach_batch.\n",
        "        :return: An EpochResult for the epoch.\n",
        "        \"\"\"\n",
        "        self.model.eval()  # set evaluation (test) mode\n",
        "        return self._foreach_batch(dl_test, self.test_batch, **kw)\n",
        "\n",
        "    def train_batch(self, index, batch_data) -> BatchResult:\n",
        "        \"\"\"\n",
        "        Runs a single batch forward through the model, calculates loss,\n",
        "        preforms back-propagation and uses the optimizer to update weights.\n",
        "        :param batch: A single batch of data  from a data loader (might\n",
        "            be a tuple of data and labels or anything else depending on\n",
        "            the underlying dataset.\n",
        "        :return: A BatchResult containing the value of the loss function and\n",
        "            the number of correctly classified samples in the batch.\n",
        "        \"\"\"\n",
        "\n",
        "        X, y = batch_data\n",
        "        if self.tensorboard_logger and self.tensorboard_log_images:\n",
        "            B = torch.zeros_like(X.squeeze())\n",
        "            C = torch.stack([B, X.squeeze(), X.squeeze()])\n",
        "            C = C.unsqueeze(dim=0)\n",
        "            images = C\n",
        "            grid = make_grid(images, normalize=True, scale_each=True)\n",
        "            self.tensorboard_logger.add_image(\"exp-%s/batch/test/images\" % self.experiment_prefix, grid, index)\n",
        "        if isinstance(X, tuple) or isinstance(X, list):\n",
        "            X = [x.to(self.device) for x in X]\n",
        "        else:\n",
        "            X = X.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "        pred = self.model(X)\n",
        "        loss = self.loss_fn(pred, y)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        score = self.objective_metric(pred, y)\n",
        "        if self.tensorboard_logger:\n",
        "            self.tensorboard_logger.add_scalar('exp-%s/batch/train/loss' % self.experiment_prefix, loss, index)\n",
        "            self.tensorboard_logger.add_scalar('exp-%s/batch/train/score' % self.experiment_prefix, score, index)\n",
        "            if index % 300 == 0:\n",
        "                for tag, value in self.model.named_parameters():\n",
        "                    tag = tag.replace('.', '/')\n",
        "                    self.tensorboard_logger.add_histogram('exp-%s/batch/train/param/%s' % (self.experiment_prefix, tag), to_np(value), index)\n",
        "                    self.tensorboard_logger.add_histogram('exp-%s/batch/train/param/%s/grad' % (self.experiment_prefix, tag), to_np(value.grad), index)\n",
        "\n",
        "        return BatchResult(loss, score)\n",
        "\n",
        "    def test_batch(self, index, batch_data) -> BatchResult:\n",
        "        \"\"\"\n",
        "        Runs a single batch forward through the model and calculates loss.\n",
        "        :param batch: A single batch of data  from a data loader (might\n",
        "            be a tuple of data and labels or anything else depending on\n",
        "            the underlying dataset.\n",
        "        :return: A BatchResult containing the value of the loss function and\n",
        "            the number of correctly classified samples in the batch.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            X, y = batch_data\n",
        "            if isinstance(X, tuple) or isinstance(X, list):\n",
        "                X = [x.to(self.device) for x in X]\n",
        "            else:\n",
        "                X = X.to(self.device)\n",
        "            y = y.to(self.device)\n",
        "            pred = self.model(X)\n",
        "            loss = self.loss_fn(pred, y)\n",
        "            score = self.objective_metric(pred, y)\n",
        "            if self.tensorboard_logger:\n",
        "                self.tensorboard_logger.add_scalar('exp-%s/batch/test/loss' % self.experiment_prefix, loss, index)\n",
        "                self.tensorboard_logger.add_scalar('exp-%s/batch/test/score' % self.experiment_prefix, score, index)\n",
        "            return BatchResult(loss, score)\n",
        "\n",
        "    @staticmethod\n",
        "    def _print(message, verbose=True):\n",
        "        \"\"\" Simple wrapper around print to make it conditional \"\"\"\n",
        "        if verbose:\n",
        "            print(message)\n",
        "\n",
        "    @staticmethod\n",
        "    def _foreach_batch(dl: DataLoader,\n",
        "                       forward_fn: Callable[[Any], BatchResult],\n",
        "                       verbose=True, max_batches=None) -> EpochResult:\n",
        "        \"\"\"\n",
        "        Evaluates the given forward-function on batches from the given\n",
        "        dataloader, and prints progress along the way.\n",
        "        \"\"\"\n",
        "        losses = []\n",
        "        num_samples = len(dl.sampler)\n",
        "        num_batches = len(dl.batch_sampler)\n",
        "\n",
        "        if max_batches is not None:\n",
        "            if max_batches < num_batches:\n",
        "                num_batches = max_batches\n",
        "                num_samples = num_batches * dl.batch_size\n",
        "\n",
        "        if verbose:\n",
        "            pbar_file = sys.stdout\n",
        "        else:\n",
        "            pbar_file = open(os.devnull, 'w')\n",
        "\n",
        "        pbar_name = forward_fn.__name__\n",
        "        with tqdm.tqdm(desc=pbar_name, total=num_batches,\n",
        "                       file=pbar_file) as pbar:\n",
        "            dl_iter = iter(dl)\n",
        "            overall_score = overall_loss = avg_score = avg_loss = counter = 0\n",
        "            min_loss = min_score = 1\n",
        "            max_loss = max_score = 0\n",
        "            for batch_idx in range(num_batches):\n",
        "                counter += 1\n",
        "                data = next(dl_iter)\n",
        "                batch_res = forward_fn(batch_idx, data)\n",
        "                if batch_res.loss > max_loss:\n",
        "                    max_loss = batch_res.loss\n",
        "                if batch_res.score > max_score:\n",
        "                    max_score = batch_res.score\n",
        "\n",
        "                if batch_res.loss < min_loss:\n",
        "                    min_loss = batch_res.loss\n",
        "                if batch_res.score < min_score:\n",
        "                    min_score = batch_res.score\n",
        "                overall_loss += batch_res.loss\n",
        "                overall_score += batch_res.score\n",
        "                losses.append(batch_res.loss)\n",
        "\n",
        "                avg_loss = overall_loss / counter\n",
        "                avg_score = overall_score / counter\n",
        "                pbar.set_description(f'{pbar_name} (Avg. loss:{avg_loss:.3f}, Avg. score:{avg_score:.3f})')\n",
        "                pbar.update()\n",
        "\n",
        "            pbar.set_description(f'{pbar_name} '\n",
        "                                 f'(Avg. Loss {avg_loss:.3f}, Min {min_loss:.3f}, Max {max_loss:.3f}), '\n",
        "                                 f'(Avg. Score {avg_score:.4f}, Min {min_score:.4f}, Max {max_score:.4f})')\n",
        "\n",
        "        return EpochResult(losses=losses, score=avg_score)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuHZm78xJeQ-"
      },
      "source": [
        "### Data Preprocessing\n",
        "  Dataset and DataLoader of train and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VSNYdz6r8t6",
        "outputId": "87795ccd-1a76-402c-e80b-7072f76835bf"
      },
      "source": [
        "# Data preparation\n",
        "# load the data see data_preprocessing notebook for more explanation\n",
        "\n",
        "def normalize(x,  mean=470, std=None):\n",
        "    mean_tansor = torch.ones_like(x) * mean\n",
        "    x -= mean_tansor\n",
        "    if std:\n",
        "        x /= std\n",
        "    return x\n",
        "\n",
        "def preprocessing(image, mask):\n",
        "    mask_transformer = transforms.Compose([\n",
        "        transforms.ToTensor()    \n",
        "    ])\n",
        "\n",
        "    image_transformer = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: normalize(x))\n",
        "    ])\n",
        "    return image_transformer(image).float(), mask_transformer(mask).float()\n",
        "\n",
        "target_folder = './data/'\n",
        "ds_train = BrainTumorDatasetMask(root=target_folder, train=True, transform=preprocessing)\n",
        "ds_test = BrainTumorDatasetMask(root=target_folder, train=False, transform=preprocessing)\n",
        "dl_train = torch.utils.data.DataLoader(ds_train, batch_size, shuffle=True)\n",
        "dl_test = torch.utils.data.DataLoader(ds_test, batch_size, shuffle=False)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data directory already exists.  if from some reason the data directory structure is wrong please remove the data dir and rerun this script\n",
            "Data directory already exists.  if from some reason the data directory structure is wrong please remove the data dir and rerun this script\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u3OHuNKTC2w"
      },
      "source": [
        "### Main Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2W8Zc6Sjr8t7",
        "outputId": "437b85b1-c596-4a13-b861-edc95476a833"
      },
      "source": [
        "# Training and evaluation\n",
        "\n",
        "# Regular UNet segmentation for all types of cancer\n",
        "# Encoder output is 1024 channels\n",
        "\n",
        "model = DeepResUNet()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = dice_loss\n",
        "success_metric = dice_coeff\n",
        "trainer = Trainer(model, criterion, optimizer, dice_coeff, device, None)\n",
        "fit_res = trainer.fit(dl_train, \n",
        "                      dl_test,\n",
        "                      num_epochs= num_epochs,\n",
        "                      checkpoints='./src/saved_models/' + model.__class__.__name__ + \"V2\")# still cannot automatically create this path..."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoints directory already exists\n",
            "--- EPOCH 1/100 ---\n",
            "train_batch (Avg. Loss 0.893, Min 0.459, Max 1.000), (Avg. Score 0.1067, Min 0.0000, Max 0.5411): 100%|██████████| 1296/1296 [20:53<00:00,  1.03it/s]\n",
            "test_batch (Avg. Loss 0.970, Min 0.917, Max 0.994), (Avg. Score 0.0304, Min 0.0055, Max 0.0831): 100%|██████████| 229/229 [02:05<00:00,  1.83it/s]\n",
            "--- EPOCH 2/100 ---\n",
            "train_batch (Avg. Loss 0.868, Min 0.417, Max 1.000), (Avg. Score 0.1316, Min 0.0000, Max 0.5827): 100%|██████████| 1296/1296 [13:37<00:00,  1.59it/s]\n",
            "test_batch (Avg. Loss 0.981, Min 0.728, Max 1.000), (Avg. Score 0.0193, Min 0.0000, Max 0.2721): 100%|██████████| 229/229 [00:47<00:00,  4.79it/s]\n",
            "--- EPOCH 3/100 ---\n",
            "train_batch (Avg. Loss 0.835, Min 0.221, Max 1.000), (Avg. Score 0.1645, Min 0.0000, Max 0.7790): 100%|██████████| 1296/1296 [13:36<00:00,  1.59it/s]\n",
            "test_batch (Avg. Loss 0.938, Min 0.291, Max 1.000), (Avg. Score 0.0620, Min 0.0000, Max 0.7094): 100%|██████████| 229/229 [00:48<00:00,  4.76it/s]\n",
            "**** Checkpoint saved ****\n",
            "--- EPOCH 4/100 ---\n",
            "train_batch (Avg. Loss 0.760, Min 0.115, Max 1.000), (Avg. Score 0.2404, Min 0.0000, Max 0.8849): 100%|██████████| 1296/1296 [13:39<00:00,  1.58it/s]\n",
            "test_batch (Avg. Loss 0.972, Min 0.573, Max 1.000), (Avg. Score 0.0281, Min 0.0000, Max 0.4273): 100%|██████████| 229/229 [00:48<00:00,  4.74it/s]\n",
            "--- EPOCH 5/100 ---\n",
            "train_batch (Avg. Loss 0.738, Min 0.130, Max 1.000), (Avg. Score 0.2621, Min 0.0000, Max 0.8702): 100%|██████████| 1296/1296 [13:41<00:00,  1.58it/s]\n",
            "test_batch (Avg. Loss 0.921, Min 0.315, Max 1.000), (Avg. Score 0.0791, Min 0.0000, Max 0.6847): 100%|██████████| 229/229 [00:48<00:00,  4.72it/s]\n",
            "**** Checkpoint saved ****\n",
            "--- EPOCH 6/100 ---\n",
            "train_batch (Avg. Loss 0.691, Min 0.107, Max 1.000), (Avg. Score 0.3088, Min 0.0000, Max 0.8933): 100%|██████████| 1296/1296 [13:42<00:00,  1.58it/s]\n",
            "test_batch (Avg. Loss 0.883, Min 0.313, Max 1.000), (Avg. Score 0.1165, Min 0.0000, Max 0.6866): 100%|██████████| 229/229 [00:48<00:00,  4.68it/s]\n",
            "**** Checkpoint saved ****\n",
            "--- EPOCH 7/100 ---\n",
            "train_batch (Avg. Loss 0.632, Min 0.084, Max 1.000), (Avg. Score 0.3683, Min 0.0000, Max 0.9163): 100%|██████████| 1296/1296 [13:43<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.857, Min 0.112, Max 1.000), (Avg. Score 0.1434, Min 0.0000, Max 0.8880): 100%|██████████| 229/229 [00:48<00:00,  4.69it/s]\n",
            "**** Checkpoint saved ****\n",
            "--- EPOCH 8/100 ---\n",
            "train_batch (Avg. Loss 0.576, Min 0.081, Max 1.000), (Avg. Score 0.4241, Min 0.0000, Max 0.9188): 100%|██████████| 1296/1296 [13:42<00:00,  1.58it/s]\n",
            "test_batch (Avg. Loss 0.826, Min 0.086, Max 1.000), (Avg. Score 0.1740, Min 0.0000, Max 0.9135): 100%|██████████| 229/229 [00:48<00:00,  4.68it/s]\n",
            "**** Checkpoint saved ****\n",
            "--- EPOCH 9/100 ---\n",
            "train_batch (Avg. Loss 0.534, Min 0.072, Max 1.000), (Avg. Score 0.4656, Min 0.0000, Max 0.9275): 100%|██████████| 1296/1296 [13:43<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.813, Min 0.077, Max 1.000), (Avg. Score 0.1870, Min 0.0000, Max 0.9226): 100%|██████████| 229/229 [00:49<00:00,  4.67it/s]\n",
            "**** Checkpoint saved ****\n",
            "--- EPOCH 10/100 ---\n",
            "train_batch (Avg. Loss 0.489, Min 0.060, Max 1.000), (Avg. Score 0.5106, Min 0.0000, Max 0.9398): 100%|██████████| 1296/1296 [13:42<00:00,  1.58it/s]\n",
            "test_batch (Avg. Loss 0.805, Min 0.104, Max 1.000), (Avg. Score 0.1946, Min 0.0000, Max 0.8953): 100%|██████████| 229/229 [00:48<00:00,  4.76it/s]\n",
            "**** Checkpoint saved ****\n",
            "--- EPOCH 11/100 ---\n",
            "train_batch (Avg. Loss 0.458, Min 0.054, Max 1.000), (Avg. Score 0.5417, Min 0.0000, Max 0.9456): 100%|██████████| 1296/1296 [13:42<00:00,  1.58it/s]\n",
            "test_batch (Avg. Loss 0.764, Min 0.113, Max 1.000), (Avg. Score 0.2358, Min 0.0000, Max 0.8866): 100%|██████████| 229/229 [00:48<00:00,  4.71it/s]\n",
            "**** Checkpoint saved ****\n",
            "--- EPOCH 12/100 ---\n",
            "train_batch (Avg. Loss 0.443, Min 0.038, Max 1.000), (Avg. Score 0.5569, Min 0.0000, Max 0.9616): 100%|██████████| 1296/1296 [13:42<00:00,  1.58it/s]\n",
            "test_batch (Avg. Loss 0.761, Min 0.058, Max 1.000), (Avg. Score 0.2391, Min 0.0000, Max 0.9424): 100%|██████████| 229/229 [00:48<00:00,  4.69it/s]\n",
            "**** Checkpoint saved ****\n",
            "--- EPOCH 13/100 ---\n",
            "train_batch (Avg. Loss 0.418, Min 0.031, Max 0.997), (Avg. Score 0.5820, Min 0.0027, Max 0.9689): 100%|██████████| 1296/1296 [13:43<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.675, Min 0.072, Max 1.000), (Avg. Score 0.3249, Min 0.0000, Max 0.9286): 100%|██████████| 229/229 [00:48<00:00,  4.68it/s]\n",
            "**** Checkpoint saved ****\n",
            "--- EPOCH 14/100 ---\n",
            "train_batch (Avg. Loss 0.398, Min 0.038, Max 1.000), (Avg. Score 0.6018, Min 0.0000, Max 0.9615): 100%|██████████| 1296/1296 [13:43<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.740, Min 0.053, Max 1.000), (Avg. Score 0.2600, Min 0.0000, Max 0.9469): 100%|██████████| 229/229 [00:48<00:00,  4.69it/s]\n",
            "--- EPOCH 15/100 ---\n",
            "train_batch (Avg. Loss 0.380, Min 0.039, Max 1.000), (Avg. Score 0.6205, Min 0.0000, Max 0.9613): 100%|██████████| 1296/1296 [13:44<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.746, Min 0.060, Max 1.000), (Avg. Score 0.2539, Min 0.0000, Max 0.9401): 100%|██████████| 229/229 [00:49<00:00,  4.65it/s]\n",
            "--- EPOCH 16/100 ---\n",
            "train_batch (Avg. Loss 0.379, Min 0.034, Max 0.992), (Avg. Score 0.6213, Min 0.0075, Max 0.9661): 100%|██████████| 1296/1296 [13:43<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.666, Min 0.062, Max 1.000), (Avg. Score 0.3344, Min 0.0000, Max 0.9378): 100%|██████████| 229/229 [00:49<00:00,  4.67it/s]\n",
            "**** Checkpoint saved ****\n",
            "--- EPOCH 17/100 ---\n",
            "train_batch (Avg. Loss 0.356, Min 0.034, Max 1.000), (Avg. Score 0.6435, Min 0.0000, Max 0.9661): 100%|██████████| 1296/1296 [13:43<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.718, Min 0.066, Max 1.000), (Avg. Score 0.2821, Min 0.0000, Max 0.9340): 100%|██████████| 229/229 [00:49<00:00,  4.67it/s]\n",
            "--- EPOCH 18/100 ---\n",
            "train_batch (Avg. Loss 0.354, Min 0.029, Max 1.000), (Avg. Score 0.6465, Min 0.0002, Max 0.9712): 100%|██████████| 1296/1296 [13:44<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.739, Min 0.067, Max 1.000), (Avg. Score 0.2611, Min 0.0000, Max 0.9327): 100%|██████████| 229/229 [00:49<00:00,  4.66it/s]\n",
            "--- EPOCH 19/100 ---\n",
            "train_batch (Avg. Loss 0.343, Min 0.041, Max 1.000), (Avg. Score 0.6573, Min 0.0000, Max 0.9591): 100%|██████████| 1296/1296 [13:44<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.666, Min 0.068, Max 1.000), (Avg. Score 0.3345, Min 0.0000, Max 0.9315): 100%|██████████| 229/229 [00:48<00:00,  4.68it/s]\n",
            "**** Checkpoint saved ****\n",
            "--- EPOCH 20/100 ---\n",
            "train_batch (Avg. Loss 0.331, Min 0.038, Max 1.000), (Avg. Score 0.6689, Min 0.0000, Max 0.9617): 100%|██████████| 1296/1296 [13:43<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.697, Min 0.051, Max 1.000), (Avg. Score 0.3030, Min 0.0000, Max 0.9490): 100%|██████████| 229/229 [00:48<00:00,  4.71it/s]\n",
            "--- EPOCH 21/100 ---\n",
            "train_batch (Avg. Loss 0.323, Min 0.033, Max 0.999), (Avg. Score 0.6775, Min 0.0011, Max 0.9673): 100%|██████████| 1296/1296 [13:43<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.663, Min 0.058, Max 1.000), (Avg. Score 0.3366, Min 0.0000, Max 0.9424): 100%|██████████| 229/229 [00:48<00:00,  4.68it/s]\n",
            "**** Checkpoint saved ****\n",
            "--- EPOCH 22/100 ---\n",
            "train_batch (Avg. Loss 0.316, Min 0.032, Max 0.954), (Avg. Score 0.6838, Min 0.0455, Max 0.9682): 100%|██████████| 1296/1296 [13:43<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.679, Min 0.048, Max 1.000), (Avg. Score 0.3210, Min 0.0000, Max 0.9518): 100%|██████████| 229/229 [00:48<00:00,  4.69it/s]\n",
            "--- EPOCH 23/100 ---\n",
            "train_batch (Avg. Loss 0.311, Min 0.028, Max 1.000), (Avg. Score 0.6894, Min 0.0000, Max 0.9724): 100%|██████████| 1296/1296 [13:43<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.633, Min 0.062, Max 1.000), (Avg. Score 0.3667, Min 0.0000, Max 0.9371): 100%|██████████| 229/229 [00:48<00:00,  4.69it/s]\n",
            "**** Checkpoint saved ****\n",
            "--- EPOCH 24/100 ---\n",
            "train_batch (Avg. Loss 0.289, Min 0.020, Max 1.000), (Avg. Score 0.7115, Min 0.0000, Max 0.9799): 100%|██████████| 1296/1296 [13:43<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.681, Min 0.056, Max 1.000), (Avg. Score 0.3186, Min 0.0000, Max 0.9439): 100%|██████████| 229/229 [00:48<00:00,  4.70it/s]\n",
            "--- EPOCH 25/100 ---\n",
            "train_batch (Avg. Loss 0.285, Min 0.021, Max 0.982), (Avg. Score 0.7150, Min 0.0185, Max 0.9788): 100%|██████████| 1296/1296 [13:43<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.632, Min 0.069, Max 1.000), (Avg. Score 0.3685, Min 0.0000, Max 0.9315): 100%|██████████| 229/229 [00:48<00:00,  4.73it/s]\n",
            "**** Checkpoint saved ****\n",
            "--- EPOCH 26/100 ---\n",
            "train_batch (Avg. Loss 0.282, Min 0.029, Max 0.979), (Avg. Score 0.7180, Min 0.0205, Max 0.9707): 100%|██████████| 1296/1296 [13:43<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.648, Min 0.056, Max 1.000), (Avg. Score 0.3520, Min 0.0000, Max 0.9440): 100%|██████████| 229/229 [00:49<00:00,  4.64it/s]\n",
            "--- EPOCH 27/100 ---\n",
            "train_batch (Avg. Loss 0.279, Min 0.024, Max 1.000), (Avg. Score 0.7213, Min 0.0000, Max 0.9763): 100%|██████████| 1296/1296 [13:44<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.658, Min 0.052, Max 1.000), (Avg. Score 0.3418, Min 0.0000, Max 0.9478): 100%|██████████| 229/229 [00:49<00:00,  4.66it/s]\n",
            "--- EPOCH 28/100 ---\n",
            "train_batch (Avg. Loss 0.264, Min 0.023, Max 1.000), (Avg. Score 0.7361, Min 0.0000, Max 0.9772): 100%|██████████| 1296/1296 [13:44<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.698, Min 0.068, Max 1.000), (Avg. Score 0.3017, Min 0.0000, Max 0.9323): 100%|██████████| 229/229 [00:49<00:00,  4.65it/s]\n",
            "--- EPOCH 29/100 ---\n",
            "train_batch (Avg. Loss 0.256, Min 0.028, Max 1.000), (Avg. Score 0.7436, Min 0.0000, Max 0.9726): 100%|██████████| 1296/1296 [13:44<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.675, Min 0.044, Max 1.000), (Avg. Score 0.3246, Min 0.0000, Max 0.9565): 100%|██████████| 229/229 [00:48<00:00,  4.68it/s]\n",
            "--- EPOCH 30/100 ---\n",
            "train_batch (Avg. Loss 0.260, Min 0.022, Max 1.000), (Avg. Score 0.7400, Min 0.0000, Max 0.9775): 100%|██████████| 1296/1296 [13:44<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.637, Min 0.041, Max 1.000), (Avg. Score 0.3628, Min 0.0000, Max 0.9585): 100%|██████████| 229/229 [00:48<00:00,  4.68it/s]\n",
            "--- EPOCH 31/100 ---\n",
            "train_batch (Avg. Loss 0.254, Min 0.026, Max 0.941), (Avg. Score 0.7456, Min 0.0587, Max 0.9742): 100%|██████████| 1296/1296 [13:44<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.566, Min 0.054, Max 1.000), (Avg. Score 0.4338, Min 0.0000, Max 0.9459): 100%|██████████| 229/229 [00:48<00:00,  4.68it/s]\n",
            "**** Checkpoint saved ****\n",
            "--- EPOCH 32/100 ---\n",
            "train_batch (Avg. Loss 0.236, Min 0.024, Max 0.992), (Avg. Score 0.7643, Min 0.0076, Max 0.9758): 100%|██████████| 1296/1296 [13:43<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.551, Min 0.051, Max 1.000), (Avg. Score 0.4488, Min 0.0000, Max 0.9494): 100%|██████████| 229/229 [00:48<00:00,  4.73it/s]\n",
            "**** Checkpoint saved ****\n",
            "--- EPOCH 33/100 ---\n",
            "train_batch (Avg. Loss 0.240, Min 0.024, Max 1.000), (Avg. Score 0.7598, Min 0.0000, Max 0.9765): 100%|██████████| 1296/1296 [13:41<00:00,  1.58it/s]\n",
            "test_batch (Avg. Loss 0.613, Min 0.048, Max 1.000), (Avg. Score 0.3866, Min 0.0000, Max 0.9524): 100%|██████████| 229/229 [00:48<00:00,  4.76it/s]\n",
            "--- EPOCH 34/100 ---\n",
            "train_batch (Avg. Loss 0.239, Min 0.028, Max 1.000), (Avg. Score 0.7611, Min 0.0000, Max 0.9716): 100%|██████████| 1296/1296 [13:41<00:00,  1.58it/s]\n",
            "test_batch (Avg. Loss 0.485, Min 0.053, Max 1.000), (Avg. Score 0.5146, Min 0.0000, Max 0.9466): 100%|██████████| 229/229 [00:49<00:00,  4.67it/s]\n",
            "**** Checkpoint saved ****\n",
            "--- EPOCH 35/100 ---\n",
            "train_batch (Avg. Loss 0.226, Min 0.026, Max 0.892), (Avg. Score 0.7744, Min 0.1076, Max 0.9737): 100%|██████████| 1296/1296 [13:42<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.577, Min 0.043, Max 1.000), (Avg. Score 0.4228, Min 0.0000, Max 0.9572): 100%|██████████| 229/229 [00:48<00:00,  4.70it/s]\n",
            "--- EPOCH 36/100 ---\n",
            "train_batch (Avg. Loss 0.217, Min 0.022, Max 1.000), (Avg. Score 0.7833, Min 0.0000, Max 0.9778): 100%|██████████| 1296/1296 [13:44<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.609, Min 0.049, Max 1.000), (Avg. Score 0.3915, Min 0.0000, Max 0.9509): 100%|██████████| 229/229 [00:49<00:00,  4.65it/s]\n",
            "--- EPOCH 37/100 ---\n",
            "train_batch (Avg. Loss 0.214, Min 0.019, Max 0.915), (Avg. Score 0.7862, Min 0.0851, Max 0.9809): 100%|██████████| 1296/1296 [13:45<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.599, Min 0.040, Max 1.000), (Avg. Score 0.4006, Min 0.0000, Max 0.9600): 100%|██████████| 229/229 [00:49<00:00,  4.63it/s]\n",
            "--- EPOCH 38/100 ---\n",
            "train_batch (Avg. Loss 0.207, Min 0.022, Max 0.798), (Avg. Score 0.7927, Min 0.2016, Max 0.9781): 100%|██████████| 1296/1296 [13:45<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.654, Min 0.031, Max 1.000), (Avg. Score 0.3462, Min 0.0000, Max 0.9694): 100%|██████████| 229/229 [00:49<00:00,  4.65it/s]\n",
            "--- EPOCH 39/100 ---\n",
            "train_batch (Avg. Loss 0.206, Min 0.019, Max 0.817), (Avg. Score 0.7944, Min 0.1828, Max 0.9814): 100%|██████████| 1296/1296 [13:44<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.442, Min 0.061, Max 1.000), (Avg. Score 0.5580, Min 0.0000, Max 0.9393): 100%|██████████| 229/229 [00:49<00:00,  4.66it/s]\n",
            "**** Checkpoint saved ****\n",
            "--- EPOCH 40/100 ---\n",
            "train_batch (Avg. Loss 0.203, Min 0.025, Max 1.000), (Avg. Score 0.7971, Min 0.0000, Max 0.9752): 100%|██████████| 1296/1296 [13:44<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.457, Min 0.040, Max 1.000), (Avg. Score 0.5433, Min 0.0000, Max 0.9598): 100%|██████████| 229/229 [00:48<00:00,  4.68it/s]\n",
            "--- EPOCH 41/100 ---\n",
            "train_batch (Avg. Loss 0.218, Min 0.022, Max 0.829), (Avg. Score 0.7815, Min 0.1713, Max 0.9784): 100%|██████████| 1296/1296 [13:43<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.497, Min 0.028, Max 1.000), (Avg. Score 0.5028, Min 0.0000, Max 0.9721): 100%|██████████| 229/229 [00:48<00:00,  4.68it/s]\n",
            "--- EPOCH 42/100 ---\n",
            "train_batch (Avg. Loss 0.189, Min 0.019, Max 0.998), (Avg. Score 0.8106, Min 0.0014, Max 0.9813): 100%|██████████| 1296/1296 [13:43<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.448, Min 0.045, Max 1.000), (Avg. Score 0.5518, Min 0.0000, Max 0.9546): 100%|██████████| 229/229 [00:49<00:00,  4.67it/s]\n",
            "--- EPOCH 43/100 ---\n",
            "train_batch (Avg. Loss 0.189, Min 0.019, Max 0.904), (Avg. Score 0.8110, Min 0.0956, Max 0.9813): 100%|██████████| 1296/1296 [13:43<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.402, Min 0.047, Max 1.000), (Avg. Score 0.5981, Min 0.0000, Max 0.9534): 100%|██████████| 229/229 [00:48<00:00,  4.69it/s]\n",
            "**** Checkpoint saved ****\n",
            "--- EPOCH 44/100 ---\n",
            "train_batch (Avg. Loss 0.180, Min 0.016, Max 0.999), (Avg. Score 0.8195, Min 0.0005, Max 0.9838): 100%|██████████| 1296/1296 [13:43<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.562, Min 0.028, Max 1.000), (Avg. Score 0.4379, Min 0.0000, Max 0.9716): 100%|██████████| 229/229 [00:49<00:00,  4.67it/s]\n",
            "--- EPOCH 45/100 ---\n",
            "train_batch (Avg. Loss 0.179, Min 0.018, Max 0.774), (Avg. Score 0.8210, Min 0.2260, Max 0.9817): 100%|██████████| 1296/1296 [13:43<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.435, Min 0.034, Max 1.000), (Avg. Score 0.5654, Min 0.0000, Max 0.9658): 100%|██████████| 229/229 [00:49<00:00,  4.67it/s]\n",
            "--- EPOCH 46/100 ---\n",
            "train_batch (Avg. Loss 0.182, Min 0.019, Max 0.801), (Avg. Score 0.8183, Min 0.1992, Max 0.9807): 100%|██████████| 1296/1296 [13:43<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.474, Min 0.044, Max 1.000), (Avg. Score 0.5258, Min 0.0000, Max 0.9560): 100%|██████████| 229/229 [00:49<00:00,  4.66it/s]\n",
            "--- EPOCH 47/100 ---\n",
            "train_batch (Avg. Loss 0.173, Min 0.020, Max 0.878), (Avg. Score 0.8270, Min 0.1216, Max 0.9797): 100%|██████████| 1296/1296 [13:44<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.547, Min 0.048, Max 1.000), (Avg. Score 0.4526, Min 0.0000, Max 0.9520): 100%|██████████| 229/229 [00:49<00:00,  4.65it/s]\n",
            "--- EPOCH 48/100 ---\n",
            "train_batch (Avg. Loss 0.170, Min 0.019, Max 0.867), (Avg. Score 0.8301, Min 0.1331, Max 0.9809): 100%|██████████| 1296/1296 [13:44<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.486, Min 0.029, Max 1.000), (Avg. Score 0.5144, Min 0.0000, Max 0.9708): 100%|██████████| 229/229 [00:49<00:00,  4.66it/s]\n",
            "--- EPOCH 49/100 ---\n",
            "train_batch (Avg. Loss 0.165, Min 0.025, Max 1.000), (Avg. Score 0.8351, Min 0.0000, Max 0.9747): 100%|██████████| 1296/1296 [13:43<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.497, Min 0.041, Max 1.000), (Avg. Score 0.5035, Min 0.0000, Max 0.9591): 100%|██████████| 229/229 [00:49<00:00,  4.65it/s]\n",
            "--- EPOCH 50/100 ---\n",
            "train_batch (Avg. Loss 0.166, Min 0.024, Max 0.882), (Avg. Score 0.8341, Min 0.1179, Max 0.9757): 100%|██████████| 1296/1296 [13:44<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.451, Min 0.041, Max 1.000), (Avg. Score 0.5492, Min 0.0000, Max 0.9593): 100%|██████████| 229/229 [00:49<00:00,  4.64it/s]\n",
            "--- EPOCH 51/100 ---\n",
            "train_batch (Avg. Loss 0.164, Min 0.021, Max 0.857), (Avg. Score 0.8363, Min 0.1433, Max 0.9793): 100%|██████████| 1296/1296 [13:44<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.430, Min 0.045, Max 1.000), (Avg. Score 0.5696, Min 0.0000, Max 0.9560): 100%|██████████| 229/229 [00:49<00:00,  4.61it/s]\n",
            "--- EPOCH 52/100 ---\n",
            "train_batch (Avg. Loss 0.151, Min 0.027, Max 0.774), (Avg. Score 0.8488, Min 0.2261, Max 0.9728): 100%|██████████| 1296/1296 [13:47<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.562, Min 0.038, Max 1.000), (Avg. Score 0.4385, Min 0.0000, Max 0.9618): 100%|██████████| 229/229 [00:49<00:00,  4.62it/s]\n",
            "--- EPOCH 53/100 ---\n",
            "train_batch (Avg. Loss 0.154, Min 0.021, Max 0.810), (Avg. Score 0.8457, Min 0.1901, Max 0.9789): 100%|██████████| 1296/1296 [13:47<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.388, Min 0.035, Max 1.000), (Avg. Score 0.6123, Min 0.0000, Max 0.9654): 100%|██████████| 229/229 [00:49<00:00,  4.60it/s]\n",
            "**** Checkpoint saved ****\n",
            "--- EPOCH 54/100 ---\n",
            "train_batch (Avg. Loss 0.154, Min 0.022, Max 0.883), (Avg. Score 0.8462, Min 0.1174, Max 0.9778): 100%|██████████| 1296/1296 [13:48<00:00,  1.56it/s]\n",
            "test_batch (Avg. Loss 0.608, Min 0.039, Max 1.000), (Avg. Score 0.3920, Min 0.0000, Max 0.9606): 100%|██████████| 229/229 [00:50<00:00,  4.57it/s]\n",
            "--- EPOCH 55/100 ---\n",
            "train_batch (Avg. Loss 0.146, Min 0.018, Max 0.837), (Avg. Score 0.8535, Min 0.1635, Max 0.9819): 100%|██████████| 1296/1296 [13:48<00:00,  1.56it/s]\n",
            "test_batch (Avg. Loss 0.529, Min 0.031, Max 1.000), (Avg. Score 0.4709, Min 0.0000, Max 0.9693): 100%|██████████| 229/229 [00:49<00:00,  4.59it/s]\n",
            "--- EPOCH 56/100 ---\n",
            "train_batch (Avg. Loss 0.153, Min 0.023, Max 0.794), (Avg. Score 0.8469, Min 0.2062, Max 0.9767): 100%|██████████| 1296/1296 [13:48<00:00,  1.56it/s]\n",
            "test_batch (Avg. Loss 0.507, Min 0.043, Max 1.000), (Avg. Score 0.4934, Min 0.0000, Max 0.9570): 100%|██████████| 229/229 [00:49<00:00,  4.59it/s]\n",
            "--- EPOCH 57/100 ---\n",
            "train_batch (Avg. Loss 0.148, Min 0.027, Max 0.729), (Avg. Score 0.8524, Min 0.2709, Max 0.9735): 100%|██████████| 1296/1296 [13:48<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.418, Min 0.023, Max 1.000), (Avg. Score 0.5824, Min 0.0000, Max 0.9772): 100%|██████████| 229/229 [00:49<00:00,  4.65it/s]\n",
            "--- EPOCH 58/100 ---\n",
            "train_batch (Avg. Loss 0.137, Min 0.024, Max 1.000), (Avg. Score 0.8631, Min 0.0000, Max 0.9757): 100%|██████████| 1296/1296 [13:45<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.510, Min 0.034, Max 1.000), (Avg. Score 0.4898, Min 0.0000, Max 0.9660): 100%|██████████| 229/229 [00:49<00:00,  4.66it/s]\n",
            "--- EPOCH 59/100 ---\n",
            "train_batch (Avg. Loss 0.137, Min 0.022, Max 0.743), (Avg. Score 0.8627, Min 0.2574, Max 0.9784): 100%|██████████| 1296/1296 [13:44<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.561, Min 0.043, Max 1.000), (Avg. Score 0.4390, Min 0.0000, Max 0.9573): 100%|██████████| 229/229 [00:48<00:00,  4.69it/s]\n",
            "--- EPOCH 60/100 ---\n",
            "train_batch (Avg. Loss 0.134, Min 0.022, Max 0.682), (Avg. Score 0.8663, Min 0.3179, Max 0.9783): 100%|██████████| 1296/1296 [13:44<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.472, Min 0.043, Max 1.000), (Avg. Score 0.5278, Min 0.0000, Max 0.9573): 100%|██████████| 229/229 [00:49<00:00,  4.67it/s]\n",
            "--- EPOCH 61/100 ---\n",
            "train_batch (Avg. Loss 0.132, Min 0.023, Max 0.646), (Avg. Score 0.8679, Min 0.3539, Max 0.9768): 100%|██████████| 1296/1296 [13:44<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.396, Min 0.027, Max 1.000), (Avg. Score 0.6040, Min 0.0000, Max 0.9729): 100%|██████████| 229/229 [00:48<00:00,  4.69it/s]\n",
            "--- EPOCH 62/100 ---\n",
            "train_batch (Avg. Loss 0.127, Min 0.018, Max 0.787), (Avg. Score 0.8735, Min 0.2129, Max 0.9819): 100%|██████████| 1296/1296 [13:43<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.574, Min 0.040, Max 1.000), (Avg. Score 0.4264, Min 0.0000, Max 0.9602): 100%|██████████| 229/229 [00:48<00:00,  4.69it/s]\n",
            "--- EPOCH 63/100 ---\n",
            "train_batch (Avg. Loss 0.124, Min 0.019, Max 0.679), (Avg. Score 0.8762, Min 0.3213, Max 0.9811): 100%|██████████| 1296/1296 [13:43<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.372, Min 0.036, Max 1.000), (Avg. Score 0.6277, Min 0.0000, Max 0.9644): 100%|██████████| 229/229 [00:48<00:00,  4.73it/s]\n",
            "**** Checkpoint saved ****\n",
            "--- EPOCH 64/100 ---\n",
            "train_batch (Avg. Loss 0.121, Min 0.020, Max 0.662), (Avg. Score 0.8791, Min 0.3376, Max 0.9800): 100%|██████████| 1296/1296 [13:43<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.563, Min 0.041, Max 1.000), (Avg. Score 0.4368, Min 0.0000, Max 0.9588): 100%|██████████| 229/229 [00:48<00:00,  4.70it/s]\n",
            "--- EPOCH 65/100 ---\n",
            "train_batch (Avg. Loss 0.123, Min 0.019, Max 0.728), (Avg. Score 0.8770, Min 0.2723, Max 0.9812): 100%|██████████| 1296/1296 [13:42<00:00,  1.58it/s]\n",
            "test_batch (Avg. Loss 0.449, Min 0.032, Max 1.000), (Avg. Score 0.5510, Min 0.0000, Max 0.9681): 100%|██████████| 229/229 [00:48<00:00,  4.74it/s]\n",
            "--- EPOCH 66/100 ---\n",
            "train_batch (Avg. Loss 0.124, Min 0.021, Max 0.892), (Avg. Score 0.8761, Min 0.1077, Max 0.9787): 100%|██████████| 1296/1296 [13:42<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.363, Min 0.036, Max 1.000), (Avg. Score 0.6371, Min 0.0000, Max 0.9645): 100%|██████████| 229/229 [00:49<00:00,  4.66it/s]\n",
            "**** Checkpoint saved ****\n",
            "--- EPOCH 67/100 ---\n",
            "train_batch (Avg. Loss 0.115, Min 0.023, Max 0.614), (Avg. Score 0.8854, Min 0.3859, Max 0.9770): 100%|██████████| 1296/1296 [13:43<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.377, Min 0.045, Max 1.000), (Avg. Score 0.6225, Min 0.0000, Max 0.9549): 100%|██████████| 229/229 [00:48<00:00,  4.68it/s]\n",
            "--- EPOCH 68/100 ---\n",
            "train_batch (Avg. Loss 0.119, Min 0.020, Max 0.621), (Avg. Score 0.8805, Min 0.3795, Max 0.9801): 100%|██████████| 1296/1296 [13:45<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.455, Min 0.053, Max 1.000), (Avg. Score 0.5450, Min 0.0000, Max 0.9473): 100%|██████████| 229/229 [00:49<00:00,  4.65it/s]\n",
            "--- EPOCH 69/100 ---\n",
            "train_batch (Avg. Loss 0.118, Min 0.019, Max 0.656), (Avg. Score 0.8823, Min 0.3436, Max 0.9809): 100%|██████████| 1296/1296 [13:46<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.413, Min 0.034, Max 1.000), (Avg. Score 0.5872, Min 0.0000, Max 0.9661): 100%|██████████| 229/229 [00:49<00:00,  4.60it/s]\n",
            "--- EPOCH 70/100 ---\n",
            "train_batch (Avg. Loss 0.116, Min 0.020, Max 0.860), (Avg. Score 0.8838, Min 0.1401, Max 0.9800): 100%|██████████| 1296/1296 [13:47<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.394, Min 0.029, Max 1.000), (Avg. Score 0.6059, Min 0.0000, Max 0.9714): 100%|██████████| 229/229 [00:49<00:00,  4.60it/s]\n",
            "--- EPOCH 71/100 ---\n",
            "train_batch (Avg. Loss 0.108, Min 0.019, Max 0.671), (Avg. Score 0.8923, Min 0.3292, Max 0.9811): 100%|██████████| 1296/1296 [13:47<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.453, Min 0.042, Max 1.000), (Avg. Score 0.5471, Min 0.0000, Max 0.9582): 100%|██████████| 229/229 [00:49<00:00,  4.63it/s]\n",
            "--- EPOCH 72/100 ---\n",
            "train_batch (Avg. Loss 0.105, Min 0.024, Max 0.627), (Avg. Score 0.8950, Min 0.3726, Max 0.9755): 100%|██████████| 1296/1296 [13:47<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.472, Min 0.034, Max 1.000), (Avg. Score 0.5285, Min 0.0000, Max 0.9664): 100%|██████████| 229/229 [00:49<00:00,  4.59it/s]\n",
            "--- EPOCH 73/100 ---\n",
            "train_batch (Avg. Loss 0.110, Min 0.022, Max 0.607), (Avg. Score 0.8897, Min 0.3931, Max 0.9775): 100%|██████████| 1296/1296 [13:47<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.606, Min 0.046, Max 1.000), (Avg. Score 0.3945, Min 0.0000, Max 0.9536): 100%|██████████| 229/229 [00:49<00:00,  4.62it/s]\n",
            "--- EPOCH 74/100 ---\n",
            "train_batch (Avg. Loss 0.119, Min 0.025, Max 0.654), (Avg. Score 0.8814, Min 0.3460, Max 0.9750): 100%|██████████| 1296/1296 [13:47<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.491, Min 0.030, Max 1.000), (Avg. Score 0.5086, Min 0.0000, Max 0.9702): 100%|██████████| 229/229 [00:49<00:00,  4.62it/s]\n",
            "--- EPOCH 75/100 ---\n",
            "train_batch (Avg. Loss 0.101, Min 0.020, Max 0.610), (Avg. Score 0.8987, Min 0.3903, Max 0.9798): 100%|██████████| 1296/1296 [13:46<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.466, Min 0.038, Max 1.000), (Avg. Score 0.5336, Min 0.0000, Max 0.9622): 100%|██████████| 229/229 [00:49<00:00,  4.62it/s]\n",
            "--- EPOCH 76/100 ---\n",
            "train_batch (Avg. Loss 0.107, Min 0.021, Max 0.614), (Avg. Score 0.8928, Min 0.3860, Max 0.9793): 100%|██████████| 1296/1296 [13:46<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.416, Min 0.028, Max 1.000), (Avg. Score 0.5838, Min 0.0000, Max 0.9724): 100%|██████████| 229/229 [00:49<00:00,  4.63it/s]\n",
            "--- EPOCH 77/100 ---\n",
            "train_batch (Avg. Loss 0.102, Min 0.022, Max 0.724), (Avg. Score 0.8976, Min 0.2761, Max 0.9777): 100%|██████████| 1296/1296 [13:46<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.400, Min 0.038, Max 1.000), (Avg. Score 0.5996, Min 0.0000, Max 0.9623): 100%|██████████| 229/229 [00:49<00:00,  4.62it/s]\n",
            "--- EPOCH 78/100 ---\n",
            "train_batch (Avg. Loss 0.104, Min 0.017, Max 0.601), (Avg. Score 0.8961, Min 0.3987, Max 0.9832): 100%|██████████| 1296/1296 [13:45<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.422, Min 0.040, Max 1.000), (Avg. Score 0.5779, Min 0.0000, Max 0.9602): 100%|██████████| 229/229 [00:49<00:00,  4.62it/s]\n",
            "--- EPOCH 79/100 ---\n",
            "train_batch (Avg. Loss 0.103, Min 0.021, Max 0.621), (Avg. Score 0.8972, Min 0.3790, Max 0.9786): 100%|██████████| 1296/1296 [13:45<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.440, Min 0.033, Max 1.000), (Avg. Score 0.5598, Min 0.0000, Max 0.9671): 100%|██████████| 229/229 [00:49<00:00,  4.63it/s]\n",
            "--- EPOCH 80/100 ---\n",
            "train_batch (Avg. Loss 0.099, Min 0.018, Max 0.570), (Avg. Score 0.9011, Min 0.4294, Max 0.9824): 100%|██████████| 1296/1296 [13:46<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.325, Min 0.025, Max 1.000), (Avg. Score 0.6747, Min 0.0000, Max 0.9748): 100%|██████████| 229/229 [00:48<00:00,  4.68it/s]\n",
            "**** Checkpoint saved ****\n",
            "--- EPOCH 81/100 ---\n",
            "train_batch (Avg. Loss 0.096, Min 0.017, Max 0.616), (Avg. Score 0.9043, Min 0.3835, Max 0.9832): 100%|██████████| 1296/1296 [13:46<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.401, Min 0.036, Max 1.000), (Avg. Score 0.5992, Min 0.0000, Max 0.9638): 100%|██████████| 229/229 [00:49<00:00,  4.64it/s]\n",
            "--- EPOCH 82/100 ---\n",
            "train_batch (Avg. Loss 0.097, Min 0.022, Max 0.583), (Avg. Score 0.9032, Min 0.4172, Max 0.9784): 100%|██████████| 1296/1296 [13:45<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.346, Min 0.031, Max 1.000), (Avg. Score 0.6539, Min 0.0000, Max 0.9687): 100%|██████████| 229/229 [00:49<00:00,  4.63it/s]\n",
            "--- EPOCH 83/100 ---\n",
            "train_batch (Avg. Loss 0.100, Min 0.025, Max 0.593), (Avg. Score 0.8999, Min 0.4071, Max 0.9749): 100%|██████████| 1296/1296 [13:47<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.557, Min 0.039, Max 1.000), (Avg. Score 0.4434, Min 0.0000, Max 0.9614): 100%|██████████| 229/229 [00:49<00:00,  4.60it/s]\n",
            "--- EPOCH 84/100 ---\n",
            "train_batch (Avg. Loss 0.098, Min 0.019, Max 0.604), (Avg. Score 0.9022, Min 0.3959, Max 0.9812): 100%|██████████| 1296/1296 [13:47<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.495, Min 0.045, Max 1.000), (Avg. Score 0.5050, Min 0.0000, Max 0.9547): 100%|██████████| 229/229 [00:49<00:00,  4.59it/s]\n",
            "--- EPOCH 85/100 ---\n",
            "train_batch (Avg. Loss 0.091, Min 0.019, Max 0.560), (Avg. Score 0.9090, Min 0.4400, Max 0.9814): 100%|██████████| 1296/1296 [13:46<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.538, Min 0.040, Max 1.000), (Avg. Score 0.4623, Min 0.0000, Max 0.9600): 100%|██████████| 229/229 [00:49<00:00,  4.64it/s]\n",
            "--- EPOCH 86/100 ---\n",
            "train_batch (Avg. Loss 0.089, Min 0.020, Max 0.552), (Avg. Score 0.9109, Min 0.4485, Max 0.9801): 100%|██████████| 1296/1296 [13:46<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.497, Min 0.027, Max 1.000), (Avg. Score 0.5032, Min 0.0000, Max 0.9727): 100%|██████████| 229/229 [00:49<00:00,  4.63it/s]\n",
            "--- EPOCH 87/100 ---\n",
            "train_batch (Avg. Loss 0.091, Min 0.020, Max 0.600), (Avg. Score 0.9094, Min 0.3999, Max 0.9801): 100%|██████████| 1296/1296 [13:46<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.435, Min 0.034, Max 1.000), (Avg. Score 0.5653, Min 0.0000, Max 0.9663): 100%|██████████| 229/229 [00:49<00:00,  4.63it/s]\n",
            "--- EPOCH 88/100 ---\n",
            "train_batch (Avg. Loss 0.091, Min 0.024, Max 0.580), (Avg. Score 0.9087, Min 0.4194, Max 0.9761): 100%|██████████| 1296/1296 [13:46<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.423, Min 0.029, Max 1.000), (Avg. Score 0.5769, Min 0.0000, Max 0.9707): 100%|██████████| 229/229 [00:49<00:00,  4.62it/s]\n",
            "--- EPOCH 89/100 ---\n",
            "train_batch (Avg. Loss 0.089, Min 0.019, Max 0.568), (Avg. Score 0.9111, Min 0.4317, Max 0.9805): 100%|██████████| 1296/1296 [13:47<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.380, Min 0.042, Max 1.000), (Avg. Score 0.6196, Min 0.0000, Max 0.9578): 100%|██████████| 229/229 [00:49<00:00,  4.59it/s]\n",
            "--- EPOCH 90/100 ---\n",
            "train_batch (Avg. Loss 0.087, Min 0.019, Max 0.621), (Avg. Score 0.9129, Min 0.3791, Max 0.9808): 100%|██████████| 1296/1296 [13:48<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.401, Min 0.036, Max 1.000), (Avg. Score 0.5992, Min 0.0000, Max 0.9644): 100%|██████████| 229/229 [00:49<00:00,  4.60it/s]\n",
            "--- EPOCH 91/100 ---\n",
            "train_batch (Avg. Loss 0.088, Min 0.021, Max 0.790), (Avg. Score 0.9115, Min 0.2096, Max 0.9787): 100%|██████████| 1296/1296 [13:47<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.486, Min 0.044, Max 1.000), (Avg. Score 0.5142, Min 0.0000, Max 0.9559): 100%|██████████| 229/229 [00:49<00:00,  4.58it/s]\n",
            "--- EPOCH 92/100 ---\n",
            "train_batch (Avg. Loss 0.090, Min 0.021, Max 0.559), (Avg. Score 0.9102, Min 0.4410, Max 0.9792): 100%|██████████| 1296/1296 [13:47<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.356, Min 0.034, Max 1.000), (Avg. Score 0.6435, Min 0.0000, Max 0.9662): 100%|██████████| 229/229 [00:49<00:00,  4.61it/s]\n",
            "--- EPOCH 93/100 ---\n",
            "train_batch (Avg. Loss 0.083, Min 0.019, Max 0.568), (Avg. Score 0.9169, Min 0.4325, Max 0.9808): 100%|██████████| 1296/1296 [13:47<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.412, Min 0.037, Max 1.000), (Avg. Score 0.5883, Min 0.0000, Max 0.9634): 100%|██████████| 229/229 [00:49<00:00,  4.60it/s]\n",
            "--- EPOCH 94/100 ---\n",
            "train_batch (Avg. Loss 0.090, Min 0.024, Max 0.853), (Avg. Score 0.9096, Min 0.1466, Max 0.9765): 100%|██████████| 1296/1296 [13:48<00:00,  1.56it/s]\n",
            "test_batch (Avg. Loss 0.359, Min 0.039, Max 1.000), (Avg. Score 0.6410, Min 0.0000, Max 0.9605): 100%|██████████| 229/229 [00:49<00:00,  4.59it/s]\n",
            "--- EPOCH 95/100 ---\n",
            "train_batch (Avg. Loss 0.085, Min 0.019, Max 0.692), (Avg. Score 0.9149, Min 0.3079, Max 0.9808): 100%|██████████| 1296/1296 [13:47<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.341, Min 0.035, Max 1.000), (Avg. Score 0.6589, Min 0.0000, Max 0.9652): 100%|██████████| 229/229 [00:49<00:00,  4.61it/s]\n",
            "--- EPOCH 96/100 ---\n",
            "train_batch (Avg. Loss 0.084, Min 0.025, Max 0.582), (Avg. Score 0.9159, Min 0.4178, Max 0.9755): 100%|██████████| 1296/1296 [13:47<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.327, Min 0.035, Max 1.000), (Avg. Score 0.6735, Min 0.0000, Max 0.9654): 100%|██████████| 229/229 [00:49<00:00,  4.61it/s]\n",
            "--- EPOCH 97/100 ---\n",
            "train_batch (Avg. Loss 0.089, Min 0.016, Max 0.528), (Avg. Score 0.9110, Min 0.4722, Max 0.9838): 100%|██████████| 1296/1296 [13:46<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.448, Min 0.035, Max 1.000), (Avg. Score 0.5522, Min 0.0000, Max 0.9648): 100%|██████████| 229/229 [00:49<00:00,  4.59it/s]\n",
            "--- EPOCH 98/100 ---\n",
            "train_batch (Avg. Loss 0.079, Min 0.018, Max 0.546), (Avg. Score 0.9209, Min 0.4544, Max 0.9819): 100%|██████████| 1296/1296 [13:46<00:00,  1.57it/s]\n",
            "test_batch (Avg. Loss 0.450, Min 0.050, Max 1.000), (Avg. Score 0.5501, Min 0.0000, Max 0.9502): 100%|██████████| 229/229 [00:49<00:00,  4.63it/s]\n",
            "--- EPOCH 99/100 ---\n",
            "train_batch (Avg. loss:0.077, Avg. score:0.923):  48%|████▊     | 628/1296 [06:40<07:05,  1.57it/s]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfd8D2Uuu2BB"
      },
      "source": [
        "### Define Result Type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MObYradfuxAA"
      },
      "source": [
        "from typing import NamedTuple, List\n",
        "\n",
        "\n",
        "class BatchResult(NamedTuple):\n",
        "    \"\"\"\n",
        "    Represents the result of training for a single batch: the loss\n",
        "    and number of correct classifications.\n",
        "    \"\"\"\n",
        "    loss: float\n",
        "    num_correct: int\n",
        "\n",
        "\n",
        "class EpochResult(NamedTuple):\n",
        "    \"\"\"\n",
        "    Represents the result of training for a single epoch: the loss per batch\n",
        "    and accuracy on the dataset (train or test).\n",
        "    \"\"\"\n",
        "    losses: List[float]\n",
        "    accuracy: float\n",
        "\n",
        "\n",
        "class FitResult(NamedTuple):\n",
        "    \"\"\"\n",
        "    Represents the result of fitting a model for multiple epochs given a\n",
        "    training and test (or validation) set.\n",
        "    The losses are for each batch and the accuracies are per epoch.\n",
        "    \"\"\"\n",
        "    num_epochs: int\n",
        "    train_loss: List[float]\n",
        "    train_acc: List[float]\n",
        "    test_loss: List[float]\n",
        "    test_acc: List[float]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB1UctWmu9d4"
      },
      "source": [
        "### Plot the training/testing accuracy/loss curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoW48dHkuXOw"
      },
      "source": [
        "import math\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#from .train_results import FitResult\n",
        "\n",
        "\n",
        "def tensors_as_images(tensors, nrows=1, figsize=(8, 8), titles=[],\n",
        "                      wspace=0.1, hspace=0.2, cmap=None):\n",
        "    \"\"\"\n",
        "    Plots a sequence of pytorch tensors as images.\n",
        "\n",
        "    :param tensors: A sequence of pytorch tensors, should have shape CxWxH\n",
        "    \"\"\"\n",
        "    assert nrows > 0\n",
        "\n",
        "    num_tensors = len(tensors)\n",
        "\n",
        "    ncols = math.ceil(num_tensors / nrows)\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize,\n",
        "                             gridspec_kw=dict(wspace=wspace, hspace=hspace),\n",
        "                             subplot_kw=dict(yticks=[], xticks=[]))\n",
        "    axes_flat = axes.reshape(-1)\n",
        "\n",
        "    # Plot each tensor\n",
        "    for i in range(num_tensors):\n",
        "        ax = axes_flat[i]\n",
        "\n",
        "        image_tensor = tensors[i]\n",
        "        assert image_tensor.dim() == 3  # Make sure shape is CxWxH\n",
        "\n",
        "        image = image_tensor.numpy()\n",
        "        image = image.transpose(1, 2, 0)\n",
        "        image = image.squeeze()  # remove singleton dimensions if any exist\n",
        "\n",
        "        # Scale to range 0..1\n",
        "        min, max = np.min(image), np.max(image)\n",
        "        image = (image-min) / (max-min)\n",
        "\n",
        "        ax.imshow(image, cmap=cmap)\n",
        "\n",
        "        if len(titles) > i and titles[i] is not None:\n",
        "            ax.set_title(titles[i])\n",
        "\n",
        "    # If there are more axes than tensors, remove their frames\n",
        "    for j in range(num_tensors, len(axes_flat)):\n",
        "        axes_flat[j].axis('off')\n",
        "\n",
        "    return fig, axes\n",
        "\n",
        "\n",
        "def dataset_first_n(dataset, n, show_classes=False, class_labels=None,\n",
        "                    random_start=True, **kw):\n",
        "    \"\"\"\n",
        "    Plots first n images of a dataset containing tensor images.\n",
        "    \"\"\"\n",
        "\n",
        "    if random_start:\n",
        "        start = np.random.randint(0, len(dataset) - n)\n",
        "        stop = start + n\n",
        "    else:\n",
        "        start = 0\n",
        "        stop = n\n",
        "\n",
        "    # [(img0, cls0), ..., # (imgN, clsN)]\n",
        "    first_n = list(itertools.islice(dataset, start, stop))\n",
        "\n",
        "    # Split (image, class) tuples\n",
        "    first_n_images, first_n_classes = zip(*first_n)\n",
        "\n",
        "    if show_classes:\n",
        "        titles = first_n_classes\n",
        "        if class_labels:\n",
        "            titles = [class_labels[cls] for cls in first_n_classes]\n",
        "    else:\n",
        "        titles = []\n",
        "\n",
        "    return tensors_as_images(first_n_images, titles=titles, **kw)\n",
        "\n",
        "\n",
        "def plot_fit(fit_res: FitResult, fig=None, log_loss=False, legend=None):\n",
        "    \"\"\"\n",
        "    Plots a FitResult object.\n",
        "    Creates four plots: train loss, test loss, train acc, test acc.\n",
        "    :param fit_res: The fit result to plot.\n",
        "    :param fig: A figure previously returned from this function. If not None,\n",
        "        plots will the added to this figure.\n",
        "    :param log_loss: Whether to plot the losses in log scale.\n",
        "    :param legend: What to call this FitResult in the legend.\n",
        "    :return: The figure.\n",
        "    \"\"\"\n",
        "    if fig is None:\n",
        "        fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16, 10),\n",
        "                                 sharex='col', sharey=False)\n",
        "        axes = axes.reshape(-1)\n",
        "    else:\n",
        "        axes = fig.axes\n",
        "\n",
        "    for ax in axes:\n",
        "        for line in ax.lines:\n",
        "            if line.get_label() == legend:\n",
        "                line.remove()\n",
        "\n",
        "    p = itertools.product(['train', 'test'], ['loss', 'acc'])\n",
        "    for idx, (traintest, lossacc) in enumerate(p):\n",
        "        ax = axes[idx]\n",
        "        attr = f'{traintest}_{lossacc}'\n",
        "        data = getattr(fit_res, attr)\n",
        "        h = ax.plot(np.arange(1, len(data) + 1), data, label=legend)\n",
        "        ax.set_title(attr)\n",
        "        if lossacc == 'loss':\n",
        "            ax.set_xlabel('Iteration #')\n",
        "            ax.set_ylabel('Loss')\n",
        "            if log_loss:\n",
        "                ax.set_yscale('log')\n",
        "                ax.set_ylabel('Loss (log)')\n",
        "        else:\n",
        "            ax.set_xlabel('Epoch #')\n",
        "            ax.set_ylabel('Accuracy (%)')\n",
        "        if legend:\n",
        "            ax.legend()\n",
        "\n",
        "    return fig, axes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "qQyBZtQKr8t7"
      },
      "source": [
        "#from cs236605.plot import plot_fit\n",
        "plot_fit(fit_res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC007gAVvILn"
      },
      "source": [
        "### Show the dice score and training/testing accuracy plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaY7sycBr8t8"
      },
      "source": [
        "print(\"Best dice score: %.2f\" % fit_res.best_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyDHgyPpr8t8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(fit_res.train_acc)\n",
        "plt.plot(fit_res.test_acc)\n",
        "plt.legend(['train score', 'test score'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDnDRWEnr8t9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}